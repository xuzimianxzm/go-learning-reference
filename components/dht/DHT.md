## Introduction

DHT全称为分布式哈希表（Distributed Hash Table），是一种分布式存储方法

- “分布式散列表”在概念上类似与传统的“散列表”，差异在于 — — “传统的散列表”主要是用于单机上的某个软件中；
- “分布式散列表”主要是用于分布式系统（此时，分布式系统的节点可以通俗理解为散列表中的 bucket）
- “分布式散列表”主要是用来存储大量的（甚至是海量的）数据。在实际使用场景中，直接对所存储的“每一个业务数据”计算散列值，然后用散列值作为 key，业务数据本身是 value。

![avatar](https://gitee.com/xuzimian/Image/raw/master/Basic/DHT/hash_key_value.jpg)

### 分布式散列表实现与协议

- Bamboo
- Bunshin
- 内容可寻址网络（Content Addressable Network）
- Chord（英语：Chord）
- DKS系统
- Kademlia
- Leopard
- MACE（英语：MACE）
- Pastry（英语：Pastry (DHT)）
- P-Grid（英语：P-Grid）
- Tapestry（英语：Tapestry (DHT)）

### 分布式散列表的应用

- BitTorrent：文件分享应用。BitTorrent可以选用DHT作为分布式Tracker。
- Warez P2P（英语：Warez P2P）：文件分享应用。
- The Circle：文件分享应用与聊天。
- CSpace：安全的沟通系统。
- Codeen（英语：Codeen）：网页缓存。
- CoralCDN（英语：CoralCDN）
- Dijjer
- eMule：文件分享应用。
- I2P：匿名网络。
- JXTA（英语：JXTA）：开放源代码的点对点平台。
- NEOnet：文件分享应用。
- Overnet：文件分享应用。

### 为什么会出现 DHT？

在 P2P 文件共享的发展史上，出现过3种不同的技术路线(三代)

#### 第1代

采用【中央服务器】的模式 — — 每个节点都需要先连接到中央服务器，然后才能查找到自己想要的文件在哪里。 这种技术的最大缺点是 — — 中央服务器成为整个 P2P 网络的【单点故障】。这类 p2p 的典型代表是 Napster。

#### 第2代

采用【广播】的模式 — — 要找文件的时候，每个节点都向自己相连的【所有节点】进行询问；被询问的节点如果不知道这个文件在哪里，就再次进行“广播”……如此往复，直至找到所需文件。 这种技术的最大缺点是 — —
会引发“广播风暴”并严重占用网络带宽，也会严重消耗节点的系统资源。即使在协议层面通过设置 TTL（time to live），限制查询过程只递归 N 轮，依然【无法】彻底解决此弊端。 因为这种手法太吓人，获得“Query
Flooding”的绰号。
![avatar](https://gitee.com/xuzimian/Image/raw/master/Basic/DHT/Query_Flooding.jpg)
这类 p2p 的典型代表是 Gnutella 的早期版本。

#### 第3代

这一代采用的技术就是今天要聊的 DHT。 通过 DHT 这个玩意儿，不但避免了第一代技术的【单点故障】，也避免了第二代技术的【广播风暴】。

### DHT 有哪些应用场景

DHT 最早用于 P2P 文件共享和文件下载（比如：BT、电驴、电骡），之后也被广泛用于某些分布式系统中，比如：

- 分布式文件系统
- 分布式缓存
- 暗网（比如：I2P、Freenet）
- 无中心的聊天工具/IM（比如：TOX）
- 无中心的微博客/microblogging（比如：Twister）
- 无中心的社交网络/SNS

### 分布式散列表（DHT）的难点

#### “无中心”导致的难点

前面提到了 DHT 的诞生，是为了解决前面两代 P2P 技术的缺陷。其中一个缺陷是“中央服务器”导致的【单点故障】。 因此 DHT 就【不能】再依靠中央服务器。而没有了中央服务器，就需要提供一系列机制来实现节点之间的通讯。

#### “海量数据”导致的难点

DHT 的很多使用场景是为了承载海量数据（PB 或更高级别）。 由于数据是海量的，每个节点只能存储（整个系统的）一小部分数据。需要把数据【均匀分摊】到每个节点。

#### “节点动态变化”导致的难点

很多 DHT 的使用场景是在公网（互联网）上，参与 DHT 的节点（主机）会出现【频繁变化】 — — 每时每刻都有新的节点上线，也会有旧的节点下线。在这种情况下，需要确保数据依然是【均匀分摊】到所有节点。
（俺特别强调一下：传统的散列表在这种情况下的困难） 前面提到：传统散列表所含的【桶数】是固定不变滴。为啥捏？ 因为传统散列表在针对 key 计算出散列值之后，需要用“散列值”和“桶数”进行某种运算（比如：取模运算），从而得到桶的编号。
如果桶的数量出现变化，就会影响到上述“取模运算”的结果，然后导致数据错乱。

#### “高效查询”导致的难点

对于节点数很多的分布式系统，如何快速定位节点，同时又不消耗太多网络资源，这也是一个挑战。 比如前面提到第二代 P2P 技术，在查找所需文件时会导致【广播风暴】。这就成为其致命弱点。 DHT
必须有更高效的查找机制。而且这种查找机制要能适应“节点动态变化”这个特点。

### 分布式散列表（DHT）如何解决上述难点？

DHT 采用如下一些机制来解决上述问题，并满足分布式系统比较苛刻的需求。

#### “散列算法”的选择

前面提到：DHT 通常是直接拿业务数据的散列值作为 key，业务数据本身作为 value。 考虑到 DHT 需要承载的数据量通常比较大，散列函数产生的“散列值范围”（keyspace）要足够大，以防止太多的碰撞。更进一步，如果
keyspace【大到一定程度】，使得“随机碰撞”的概率小到忽略不计，就有助于简化 DHT 的系统设计。 通常的 DHT 都会采用大于等于 128 比特的散列值（2128 比 “地球上所有电子文档总数” 还要大【很多数量级】）。

#### 同构的“node ID”与“data key”

DHT 属于分布式系统的一种。既然是分布式系统，意味着存在【多个】节点（电脑主机）。在设计分布式系统的时候，一种常见的做法是：给每一个节点（node）分配【唯一的】ID。有了这个节点 ID（node ID），在系统设计上的好处是 — —
对分布式系统所依赖的物理网络的【解耦】。 很多 DHT 的设计会让“node ID”采用跟“data key”【同构】的散列值。这么搞的好处是： 1、当散列值空间足够大的时候，随机碰撞忽略不计，因此也就确保了 node ID 的唯一性
2、可以简化系统设计 — — 比如简化路由算法（下面会提及）

#### “拓扑结构”的设计

作为分布式系统，DHT 必然要定义某种拓扑结构；有了拓扑结构，自然就要设计某种“路由算法”。 如果某个 DHT 采用前面所说的 — — “node ID”与“data key”【同构】 — — 那么很自然的就会引入“Key-based
routing”。 请注意，这【不是】某个具体的路由算法，而只是某种【风格】。采用这种风格来设计路由机制，好处是：key 本身已经提供了足够多的路由信息。
当某个分布式系统具有自己的拓扑结构，它本身成为一个“覆盖网络”（洋文叫“Overlay Network”）。所谓的“覆盖网络”，通俗地说就是“网络之上的网络”。对于大部分 DHT
而言，它们是基于互联网之上的“覆盖网络”，它们的数据通讯是依赖下层的互联网来实现的。 前面提到的“node ID”，其【解耦】的作用就体现在 — — 分布式系统在设计拓扑结构和路由算法时，只需要考虑 node
ID，而不用考虑其下层网络的属性（比如：协议类型、IP 地址、端口号）。

#### “路由算法”的权衡

由于 DHT 中的节点数可能非常多（比如：几十万、几百万），而且这些节点是动态变化的。因此就【不可能】让每一个节点都记录所有其它节点的信息。实际情况是：每个节点通常只知道少数一些节点的信息。
这时候就需要设计某种路由算法，尽可能利用已知的节点来转发数据。“路由算法”这玩意儿很重要，直接决定了 DHT 的速度和资源消耗。 在确定了路由算法之后，还需要做一个两难的权衡 — — “路由表的大小”。
路由表越大，可以实现越短（跳数越少）的路由；缺点是：（由于节点动态变化）路由表的维护成本也就越高。 路由表数越小，其维护成本越小；缺点是：路由就会变长（跳数变多）。

#### 距离算法

某些 DHT 系统还会定义一种“距离算法”，用来计算：“节点之间的距离”、“数据之间的距离”、“节点与数据的距离”。 请注意：此处所说的“距离”属于【逻辑层面】，对应的是 DHT
自己的拓扑结构；它与地理位置【无关】，也与互联网的拓扑结构【无关】。 写到这里，某些聪明的读者就会明白：为啥前面要强调 — — “node ID”与“data
key”【同构】。当这两者【同构】，就可以使用【同一种“距离算法”】；反之，如果这两者不同构，多半要引入几种不同的“距离算法”。

#### 数据定位

有了前面这一大砣东西作为铺垫，现在就可以来谈谈“数据定位”啦。对 DHT 而言，这是最关键的东东。 DHT 与传统的散列表在【功能】上是类似的。说白了， 他们最关键的功能只有两个 — — “保存数据”和“获取数据”。

## Kademlia（Kad）协议 简介

Kad 诞生于2002年，由纽约大学的两个牛人（Petar Maymounkov & David Mazières）共同设计，它可以在容易出错的环境（比如节点会毫无征兆地下线）中建立一张分布式散列表。。 Kad 的原理比 Chord
稍微晦涩一些（涉及一点点数据结构的知识）。实际应用的 DHT 大部分都采用 Kad 及其变种。 比如几种知名的 P2P 下载（BT、 eDonkey/电驴、eMule/电骡）的 DHT 都是基于 Kad；知名的 I2P 暗网也依赖 Kad.

### 拓扑结构 — — 二叉树

#### 散列值的预处理

Kad 也采用了“node ID 与 data key 同构”的设计思路。然后 Kad 采用某种算法把 key 映射到一个二叉树，每一个 key 都是这个二叉树的【叶子】。 在映射之前，先做一下预处理。

1. 先把 key 以二进制形式表示。
2. 把每一个 key 缩短为它的【最短唯一前缀】。

#### 为啥要搞“最短唯一前缀”？

Kad 使用 160比特 的散列算法（比如 SHA1），完整的 key 用二进制表示有 160 个数位（bit）。 首先，实际运行的 Kad 网络，即使有几百万个节点，相比 keyspace（2160）也只是很小很小很小的一个子集。
其次，由于散列函数的特点，key 的分布是【高度随机】的。因此也是【高度离散】的——任何两个 key 都【不会】非常临近。 所以，使用“最短唯一前缀”来处理 key 的二进制形式，得到的结果就会很短（比特数远远小于 160）。

#### 散列值的映射

完成上述的预处理后，接下来的映射规则是：

1. 先把 key 以二进制形式表示，然后从高位到低位依次处理。
2. 二进制的第 n 个 bit 就对应了二叉树的第 n 层
3. 如果该位是 1，进入左子树，是 0 则进入右子树（这只是人为约定，反过来处理也可以）
4. 全部数位都处理完后，这个 key 就对应了二叉树上的某个【叶子】

![avatar](https://gitee.com/xuzimian/Image/raw/master/Basic/DHT/Shortest_unique_prefix.png)

#### 距离算法 — — 异或（XOR）

接下来要聊的是 Kad【最精妙之处】 — — 采用 XOR（按位异或操作）算法计算 key 之间的“距离”。 这种搞法使得它具备了类似于“几何距离”的某些特性（下面用 ⊕ 表示 XOR）
(A ⊕ B) == (B ⊕ A)XOR 符合“交换律”，具备对称性。相比之下，Chord 的距离算法不对称(A ⊕ A) == 0反身性，自身距离为零(A ⊕ B) > 0【不同】的两个 key 之间的距离必大于零(A ⊕ B) + (
B ⊕ C) >= (A ⊕ C)三角不等式

#### 路由机制

##### 二叉树的拆分

对每一个节点，都可以【按照自己的视角】对整个二叉树进行拆分。 拆分的规则是：先从根节点开始，把【不包含】自己的那个子树拆分出来；然后在剩下的子树再拆分不包含自己的下一层子树；以此类推，直到最后只剩下自己。 Kad 默认的散列值空间是 m =
160（散列值有 160 bits），因此拆分出来的子树【最多】有 160 个（考虑到实际的节点数【远远小于】2160，子树的个数会明显小于 160）。 对于每一个节点而言，当它以自己的视角完成子树拆分后，会得到 n
个子树；对于每个子树，如果它都能知道里面的一个节点，那么它就可以利用这 n 个节点进行递归路由，从而到达整个二叉树的【任何一个】节点（考虑到篇幅，具体的数学证明就不贴出来了）
![avatar](https://gitee.com/xuzimian/Image/raw/master/Basic/DHT/Splitting_of_Binary_Tree.png)

（示意图：二叉树的拆分。注：图中的“第三”与“第四”应对调。非常感谢热心读者在评论区指正！）

##### K-桶（K-bucket）

前面说了，每个节点在完成子树拆分后，只需要知道每个子树里面的一个节点，就足以实现全遍历。但是考虑到健壮性（请始终牢记：分布式系统的节点是动态变化滴），光知道【一个】显然是不够滴，需要知道【多个】才比较保险。 所以 Kad
论文中给出了一个“K-桶（K-bucket）”的概念。也就是说：每个节点在完成子树拆分后，要记录每个子树里面的 K 个节点。这里所说的 K 值是一个【系统级】的常量。由使用 Kad 的软件系统自己设定（比如 BT 下载使用的 Kad
网络，K 设定为 8）。 这个“K-桶”其实就是【路由表】。对于某个节点而言，如果【以它自己为视角】拆分了 n 个子树，那么它就需要维护 n 个路由表，并且每个路由表的【上限】是 K。 说 K 只是一个【上限】，是因为有两种情况使得 K
桶的尺寸会小于 K。

1. 距离越近的子树就越小。如果整个子树【可能存在的】节点数小于 K，那么该子树的 K 桶尺寸永远也不可能达到 K。
2. 有些子树虽然实际上线的节点数超过 K，但是因为种种原因，没有收集到该子树足够多的节点，这也会使得该子树的 K 桶尺寸小于 K。

![avatar](https://gitee.com/xuzimian/Image/raw/master/Basic/DHT/k_2_sub_tree.png)
（示意图：K = 2 的路由表）

![avatar](https://gitee.com/xuzimian/Image/raw/master/Basic/DHT/Routing_process.png)
（示意图：路由过程）

##### K-桶（K-bucket）的刷新机制

刷新机制大致有如下几种：

1. 主动收集节点 任何节点都可以主动发起“查询节点”的请求（对应于协议类型 FIND_NODE），从而刷新 K 桶中的节点信息（下面聊“节点的加入”时，会提及这种）
2. 被动收集节点 如果收到其它节点发来的请求（协议类型 FIND_NODE 或 FIND_VALUE），会把对方的 ID 加入自己的某个 K 桶中。
3. 探测失效节点 Kad 还是支持一种探测机制（协议类型 PING），可以判断某个 ID 的节点是否在线。因此就可以定期探测路由表中的每一个节点，然后把下线的节点从路由表中干掉。

##### “并发请求”与“α 参数”

“K桶”的这个设计思路【天生支持并发】。因为【同一个】“K桶”中的每个节点都是平等的，没有哪个更特殊；而且对【同一个】“K桶”中的节点发起请求，互相之间没有影响（无耦合）。 所以 Kad 协议还引入了一个“α参数/α因子”，默认设置为
3，使用 Kad 的软件可以在具体使用场景中调整这个“α因子”。 当需要路由到某个“子树”，会从该子树对应的“K桶”中挑选【α个节点】，然后对这几个节点【同时】发出请求。 这么做有啥好处捏？俺在本文末尾聊“性能”和“安全性”时会具体介绍。

#### 节点的加入

1. 任何一个新来的节点（假设叫 A），需要先跟 DHT 中已有的任一节点（假设叫 B）建立连接。
2. A 随机生成一个散列值作为自己的 ID（对于足够大的散列值空间，ID 相同的概率忽略不计）
3. A 向 B 发起一个查询请求（协议类型 FIND_NODE），请求的 ID 是自己（通俗地说，就是查询自己）
4. B 收到该请求之后，（如前面所说）会先把 A 的 ID 加入自己的某个 K 桶中。 然后，根据 FIND_NODE 协议的约定，B 会找到【K个】最接近 A 的节点，并返回给 A。 （B 怎么知道哪些节点接近 A 捏？这时候，【用
   XOR 表示距离】的算法就发挥作用啦）
5. A 收到这 K 个节点的 ID 之后，（仅仅根据这批 ID 的值）就可以开始初始化自己的 K 桶。
6. 然后 A 会继续向刚刚拿到的这批节点发送查询请求（协议类型 FIND_NODE），如此往复（递归），直至 A 建立了足够详细的路由表。

#### 节点的退出

与 Chord 不同，Kad 对于节点退出没有额外的要求（没有“主动退出”的说法）。 所以，Kad 的节点想离开 DHT 网络【不】需要任何操作（套用徐志摩的名言：悄悄的我走了，正如我悄悄的来）

## KRPC 协议

KRPC是BitTorrent在Kademlia理论基础之上定义的一个通信消息格式协议，主要用来支持peer节点的获取(get_peer)和peer节点的声明(announce_peer)，以及判活心跳(ping)、节点寻址(
find_node)，它在find_node的原理上和DHT是一样的，同时增加了get_peer/ announce_peer/ping协议的支持

KRPC 协议是一个由 UDP 数据包和其包裹的经过 B 编码字典组成的简洁的 RPC 协议。每次查询只会有一次响应，没有重发/重试这种机制。KRPC 协议的基础部分和 Kademila 协议一致， 比如节点的 ID 都是 160
位的二进制数，使用异或算法计算距离和使用相同方法构建路由表。不同的地方在于 KRPC 协议更改了部分 RPC 过程，并将这些 RPC 集合命名为 KRPC 协议。

下面是三种消息类型:

- query（查询）
- response（响应）
- error（错误）

下面是四类查询:

- ping
- find_node
- get_peer
- announce_peer

KRPC 消息是一个经过 B 编码的字典（dictionary），每个消息至少有三个 key，分别表示”事务 ID“、”消息类型“ 和 ”客户端名称和版本“。根据消息的不同可以有更多的 key。

### bencode编码

bencode 有 4 种数据类型: string, integer, list 和 dictionary

### KRPC字典基本组成元素

一条 KRPC 消息即可能是request，也可能是response，由一个独立的字典组成:

```markdown
1. t关键字: 每条消息都包含 t 关键字，它是一个代表了 transaction ID 的字符串。transaction ID 由请求节点产生，并且回复中要包含回显该字段(挑战-响应模型)，
   所以回复可能对应一个节点的多个请求。transaction ID 应当被编码为一个短的二进制字符串，比如 2 个字节，这样就可以对应 2^16 个请求
2. y关键字: 它由一个字节组成，表明这个消息的类型。y 对应的值有三种情况
    1. q 表示请求(请求Queries): q类型的消息它包含 2 个附加的关键字 q 和 a 1.1) 关键字 q: 是字符串类型，包含了请求的方法名字(
       get_peers/announce_peer/ping/find_node)
       1.2) 关键字 a: 一个字典类型包含了请求所附加的参数(info_hash/id..)
    2. r 表示回复(回复 Responses): 包含了返回的值。发送回复消息是在正确解析了请求消息的基础上完成的，包含了一个附加的关键字 r。关键字 r 是字典类型 2.1) id: peer节点id号或者下一跳DHT节点
       2.2) nodes": ""
       2.3) token: token
    3. e 表示错误(错误 Errors): 包含一个附加的关键字 e，关键字 e 是列表类型 3.1) 第一个元素是数字类型，表明了错误码，当一个请求不能解析或出错时，错误包将被发送。下表描述了可能出现的错误码 201: 一般错误
       202: 服务错误 203: 协议错误，比如不规范的包，无效的参数，或者错误的 toke 204: 未知方法 3.2) 第二个元素是字符串类型，表明了错误信息
```

### 请求Query具体协议

所有的请求都包含一个关键字 id，它包含了请求节点的节点 ID。所有的回复也包含关键字id，它包含了回复节点的节点 ID

#### ping

检测节点是否可达，请求包含一个参数id，代表该节点的nodeID。对应的回复也应该包含回复者的nodeID

```shell
ping Query = {"t":"aa", "y":"q", "q":"ping", "a":{"id":"abcdefghij0123456789"}}
bencoded = d1:ad2:id20:abcdefghij0123456789e1:q4:ping1:t2:aa1:y1:qe

Response = {"t":"aa", "y":"r", "r": {"id":"mnopqrstuvwxyz123456"}}
bencoded = d1:rd2:id20:mnopqrstuvwxyz123456e1:t2:aa1:y1:re
```

#### find_node

find_node 被用来查找给定 ID 的DHT节点的联系信息，该请求包含两个参数id( 代表该节点的nodeID) 和target。回复中应该包含被请求节点的路由表中距离 target最接近的K个nodeID以及对应的nodeINFO

```shell
find_node Query = {"t":"aa", "y":"q", "q":"find_node", "a": {"id":"abcdefghij0123456789", "target":"mnopqrstuvwxyz123456"}}
# "id" containing the node ID of the querying node, and "target" containing the ID of the node sought by the queryer. 
bencoded = d1:ad2:id20:abcdefghij01234567896:target20:mnopqrstuvwxyz123456e1:q9:find_node1:t2:aa1:y1:qe

Response = {"t":"aa", "y":"r", "r": {"id":"0123456789abcdefghij", "nodes": "def456..."}}
bencoded = d1:rd2:id20:0123456789abcdefghij5:nodes9:def456...e1:t2:aa1:y1:re
```

- find_node 请求包含 2 个参数，第一个参数是 id，包含了请求节点的ID。第二个参数是 target，包含了请求者正在查找的节点的 ID.

- 当一个节点接收到了 find_node 的请求，他应该给出对应的回复，回复中包含 2 个关键字 id(被请求节点的id) 和 nodes，nodes 是字符串类型，包含了被请求节点的路由表中最接近目标节点的 K(8)
  个最接近的节点的联系信息(被请求方每次都统一返回最靠近目标节点的节点列表K捅)

```shell
参数: {"id" : "<querying nodes id>", "target" : "<id of target node>"}
回复: {"id" : "<queried nodes id>", "nodes" : "<compact node info>"}
```

#### get_peers

1. get_peers 请求包含 2 个参数(id请求节点ID，info_hash代表torrent文件的infohash，infohash为种子文件的SHA1哈希值，也就是磁力链接的btih值)
2. response get_peer:
    1) 如果被请求的节点有对应 info_hash 的 peers，他将返回一个关键字 values，这是一个列表类型的字符串。每一个字符串包含了 "CompactIP-address/portinfo" 格式的 peers 信息(
       即对应的机器ip/port信息)(peer的info信息和DHT节点的info信息是一样的)
    2) 如果被请求的节点没有这个 infohash 的 peers，那么他将返回关键字 nodes(需要注意的是，如果该节点没有对应的infohash信息，而只是返回了nodes，则请求方会认为该节点是一个"可疑节点"
       ，则会从自己的路由表K捅中删除该节点)，这个关键字包含了被请求节点的路由表中离 info_hash 最近的 K 个节点(我这里没有该节点，去别的节点试试运气)，使用 "Compactnodeinfo"
       格式回复。在这两种情况下，关键字 token 都将被返回。token 关键字在今后的 annouce_peer 请求中必须要携带。token 是一个短的二进制字符串

```shell
get_peers Query = {"t":"aa", "y":"q", "q":"get_peers", "a": {"id":"abcdefghij0123456789", "info_hash":"mnopqrstuvwxyz123456"}}
bencoded = d1:ad2:id20:abcdefghij01234567899:info_hash20:mnopqrstuvwxyz123456e1:q9:get_peers1:t2:aa1:y1:qe

Response with peers = {"t":"aa", "y":"r", "r": {"id":"abcdefghij0123456789", "token":"aoeusnth", "values": ["axje.u", "idhtnm"]}}
bencoded = d1:rd2:id20:abcdefghij01234567895:token8:aoeusnth6:valuesl6:axje.u6:idhtnmee1:t2:aa1:y1:re

Response with closest nodes = {"t":"aa", "y":"r", "r": {"id":"abcdefghij0123456789", "token":"aoeusnth", "nodes": "def456..."}}
bencoded = d1:rd2:id20:abcdefghij01234567895:nodes9:def456...5:token8:aoeusnthe1:t2:aa1:y1:re

```

> infohash = 1619ecc9373c3639f4ee3e261638f29b33a6cbd6，正是磁力链接magnet:?xt=urn:btih:1619ecc9373c3639f4ee3e261638f29b33a6cbd6&dn;=ubuntu-14.10-desktop-i386.iso中的btih值

#### announce_peer

这个请求用来表明发出 announce_peer 请求的节点，正在某个端口下载 torrent 文件

announce_peer 包含 4 个参数:

1. 第一个参数是 id: 包含了请求节点的 ID
2. 第二个参数是 info_hash: 包含了 torrent 文件的 infohash
3. 第三个参数是 port: 包含了整型的端口号，表明 peer 在哪个端口下载
4. 第四个参数数是 token: 这是在之前的 get_peers 请求中收到的回复中包含的。收到 announce_peer 请求的节点必须检查这个 token 与之前我们回复给这个节点 get_peers 的 token 是否相同(
   也就说，所有下载者/发布者都要参与检测新加入的发布者是否伪造了该资源，但是这个机制有一个问题，如果最开始的那个发布者就伪造，则整条链路都是一个伪造的错的资源infohash信息了)
   如果相同，那么被请求的节点将记录发送 announce_peer 节点的 IP 和请求中包含的 port 端口号在 peer 联系信息中对应的 infohash 下，这意味着一个一个事实:
   当前这个资源有一个新的peer提供者了，下一次有其他节点希望或者这个资源的时候，会把这个新的(前一次请求下载资源的节点)也当作一个peer返回给请求者，这样，资源的提供者就越来越多，资源共享速度就越来越快

